<h1>SOC - 2024 : Retro Learning</h1>
<h2> Exploring reinforcement learning through retro games</h2>
<hr>
<h3>Week 3</h3>
This week deals with the implementation of DQNs and CNNs both of which are essential when using RL in games. The DQN helps set the value function to generate maximum reward while the CNN is used to analyze frames of the game and generate meaningful data.
<br>
<br>

In order to familiarize yourself with the theory behind DQN, we recommend that you go through the following:
<h3>Distributional Reinforcement Learning </h3><h4><i>Marc G.  Bellemare, Will Dabney, Mark Rowland</i></h4>

https://direct.mit.edu/books/oa-monograph/chapter-pdf/2111090/c011200_9780262374026.pdf 

Just this chapter is enough however you are welcome to go through the book if it interests you:

https://direct.mit.edu/books/book-pdf/2111075/book_9780262374026.pdf

This pdf elaborates on dueling NNs which is what we aim to implement next week:
<h3>Dueling Network Architectures for Deep Reinforcement Learning </h3><h4><i>Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hesselt, Marc Lanctot, Nando de Freitas</i></h4>

https://arxiv.org/pdf/1511.06581

Apart from this we have uploaded the code for a DQN for a simple cartpole environment. Please go through the code and write a similar code for any other game of your choice. The link below is the documentation for the gym library in Python:

https://www.gymlibrary.dev/index.html

We have also provided code for a simple CNN. Kindly go through it and fill in the missing parts. Here are some additional resources for CNNS:

CNNs Visualized: https://www.youtube.com/watch?v=pj9-rr1wDhM

What is Convolution: https://youtu.be/KuXjwB4LzSA?si=Fd04eDMj6QvTfFEz

A website to visualize convolutions: https://deeplizard.com/resource/pavq7noze2

Compile all of your codes in a folder titled Week 3 and upload it on Git Hub.

